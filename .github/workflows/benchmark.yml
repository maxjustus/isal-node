name: Performance Benchmarks

on:
  workflow_dispatch:  # Allow manual triggering
  schedule:
    - cron: '0 6 * * 1'  # Run weekly on Mondays at 6 AM UTC

jobs:
  benchmark:
    strategy:
      matrix:
        settings:
          - host: macos-latest
            target: aarch64-apple-darwin
            name: "macOS ARM64"
            
          - host: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            name: "Linux x64"

    name: Benchmark on ${{ matrix.settings.name }}
    runs-on: ${{ matrix.settings.host }}
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install system dependencies (macOS)
        if: matrix.settings.host == 'macos-latest'
        run: brew install autoconf automake libtool nasm yasm
      
      - name: Install system dependencies (Ubuntu)
        if: matrix.settings.host == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install build-essential autoconf automake libtool nasm yasm -y
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable
          targets: ${{ matrix.settings.target }}
      
      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          key: ${{ matrix.settings.target }}
      
      - name: Install dependencies
        run: npm ci
      
      - name: Build native module (optimized)
        run: npm run build
        env:
          RUSTFLAGS: '-C target-feature=-crt-static -C target-cpu=native'
      
      - name: Run comprehensive benchmarks
        run: |
          echo "# Performance Benchmark Report" > benchmark_report.md
          echo "Platform: ${{ matrix.settings.name }}" >> benchmark_report.md
          echo "Date: $(date)" >> benchmark_report.md
          echo "Commit: ${{ github.sha }}" >> benchmark_report.md
          echo "" >> benchmark_report.md
          
          echo "## Quick Benchmark" >> benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          npm run benchmark:quick 2>&1 | tee -a benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          echo "" >> benchmark_report.md
          
          echo "## Async Benchmark" >> benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          npm run benchmark:async 2>&1 | tee -a benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          echo "" >> benchmark_report.md
          
          echo "## Full Benchmark Suite" >> benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
          npm run benchmark 2>&1 | tee -a benchmark_report.md
          echo "\`\`\`" >> benchmark_report.md
      
      - name: Add to step summary
        run: cat benchmark_report.md >> $GITHUB_STEP_SUMMARY
      
      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ matrix.settings.target }}
          path: benchmark_report.md
          retention-days: 90
      
      - name: Save benchmark data
        run: |
          # Extract key metrics for tracking over time
          mkdir -p benchmark-data
          echo "timestamp,platform,commit,test,ops_per_sec,mb_per_sec" > benchmark-data/results.csv
          # This would need custom parsing based on your benchmark output format
          # For now, just save the raw output
          cp benchmark_report.md benchmark-data/
      
      - name: Upload benchmark data
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-data-${{ matrix.settings.target }}
          path: benchmark-data/
          retention-days: 365  # Keep for a year for trend analysis